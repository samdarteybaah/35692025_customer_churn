# -*- coding: utf-8 -*-
"""35692025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1irC6dJJwz3xdjf_vxGtnpRxMON15s-Cy

**A CUSTOMER CHURN PREDICTION AI MODEL**
****

This is an AI model developed by Samuel Dartey - Baah, to predict the kind of customers most likely to churn on telecom operators based on a given dataset.
"""

from google.colab import drive
drive.mount('/content/drive')

# installing scikeras to import KerasClassifier
!pip install scikeras
!pip install keras-tuner

#importing necessary libraries which would be used
import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
import kerastuner as kt
from keras.models import Model
from keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from tensorflow import keras
from sklearn.impute import SimpleImputer
from keras.models import Sequential
from keras.layers import Dense, Input
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler

#importing the data set
customer_churn_df = pd.read_csv('/content/drive/MyDrive/CustomerChurn_dataset.csv')

customer_churn_df.head()

"""**Extracting Relevant Features Needed From The Dataset**
****
"""

# removing unnecessary colums in the data set
customer_churn_df.drop(columns=['customerID'], inplace=True)

#checking to see if there are any null values
customer_churn_df.isnull().sum()

#checking the data types of the various values
customer_churn_df.info()

# converting total charges column into float
customer_churn_df['TotalCharges'] = pd.to_numeric(customer_churn_df['TotalCharges'], errors='coerce', downcast='float')

number_of_null_total_Charges = customer_churn_df['TotalCharges'].isnull().sum()
print (f'There are {number_of_null_total_Charges} null values in the TotalCharges column')

# reshaping the data to an array for the SimpleImputer
total_charges_array= customer_churn_df['TotalCharges'].values.reshape(-1, 1)

# using simple imputer to fill the null values in the data set
sc=SimpleImputer(strategy='mean')
customer_churn_df['TotalCharges'] = sc.fit_transform(total_charges_array)

# subsetting some groups of values the columns
customer_churn_df.replace('No internet Service', 'No', inplace = True)
customer_churn_df.replace('No internet service', 'No', inplace = True)
customer_churn_df.replace('No phone Service', 'No', inplace = True)

# all columns including TotalCharges
customer_churn_df.isnull().sum()

customer_churn_df.info()

# creating a new data set for the encoded values
encoded_customer_churn_df = customer_churn_df.copy()

# converting caterogical values to numbers using label encoding
label_encoder = LabelEncoder()

encoded_customer_churn_df['gender' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['gender'])
encoded_customer_churn_df['Partner' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['Partner'])
encoded_customer_churn_df['Dependents' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['Dependents'])
encoded_customer_churn_df['PhoneService' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['PhoneService'])
encoded_customer_churn_df['MultipleLines' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['MultipleLines'])
encoded_customer_churn_df['InternetService' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['InternetService'])
encoded_customer_churn_df['OnlineSecurity' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['OnlineSecurity'])
encoded_customer_churn_df['OnlineBackup' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['OnlineBackup'])
encoded_customer_churn_df['DeviceProtection' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['DeviceProtection'])
encoded_customer_churn_df['TechSupport' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['TechSupport'])
encoded_customer_churn_df['StreamingTV' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['StreamingTV'])
encoded_customer_churn_df['StreamingMovies' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['StreamingMovies'])
encoded_customer_churn_df['Contract' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['Contract'])
encoded_customer_churn_df['PaperlessBilling' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['PaperlessBilling'])
encoded_customer_churn_df['PaymentMethod' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['PaymentMethod'])
encoded_customer_churn_df['Churn' + '_encoded'] = label_encoder.fit_transform(encoded_customer_churn_df['Churn'])

# drop the original columns which have now been encoded
encoded_customer_churn_df.drop(columns=['gender', 'Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod','Churn'], inplace= True)

encoded_customer_churn_df

encoded_customer_churn_df.info()
print()
print(('Now all columns are numeric and filled'))

# using random forest to get the most important features
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = encoded_customer_churn_df.drop(columns=['Churn_encoded'])
y = encoded_customer_churn_df['Churn_encoded']

# splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# creating a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=5, n_estimators=100)
rf_classifier.fit(X_train_scaled, y_train)

# getting the relevant features
feature_importances = rf_classifier.feature_importances_

# creating a data set of the most relevant features
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# getting the  6 most relevant features
needed_features = feature_importance_df.head(6)['Feature'].tolist()

print(f'These are the most necessary features: {needed_features}')

"""**Exploratory Data Analysis**
****
"""

# using graphs to gain more insights about the data

sns.boxplot(x='Churn', y='MonthlyCharges', data= customer_churn_df )
plt.show()
print('This shows people are more likely to churn if charges are around 88.00 dollars or more')
print()

sns.countplot(x='SeniorCitizen', hue='Churn', data = customer_churn_df)
plt.show()
print('This shows more senior citizens are likely to Churn')
print()

sns.countplot(x='PaymentMethod', hue='Churn', data = customer_churn_df)
plt.show()
print('This shows people with electronic checks Churn more, than other payment types, and people with automatic payment systems churn less')
print()

sns.countplot(x='MultipleLines', hue='Churn', data = customer_churn_df)
plt.show()
print('This shows people without multiple lines churn less')
print()

sns.catplot(x='SeniorCitizen', hue='Churn', col='PaymentMethod', data=customer_churn_df, kind='count', height=4, aspect=0.8)
plt.show()
print('Senior citizens using electronic checks are more likely to Churn')

"""**Training The Data Using MLP**
****
"""

# dividing data set into X and Y
 X = encoded_customer_churn_df[needed_features]
 y = encoded_customer_churn_df['Churn_encoded']

# scaling the X
from sklearn.preprocessing import StandardScaler
StandardScaler = StandardScaler()

X = StandardScaler.fit_transform(X.copy())
X = pd.DataFrame(X, columns = needed_features)

# splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)

# getting the number of neurons to be used
number_of_neurons = X_train.shape[1]

# create a functional api with keras
from keras.src.losses import binary_crossentropy

def create_model(activation='relu',hidden_layer_sizes=(5, 5), input_shape=(number_of_neurons)):
    input_layer = Input(shape=input_shape)
    hidden_layer1 = Dense(hidden_layer_sizes[0], activation='relu')(input_layer)
    hidden_layer2 = Dense(hidden_layer_sizes[1], activation='relu')(hidden_layer1)
    output_layer = Dense(1, activation='sigmoid')(hidden_layer2)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

    return model


model = create_model()
model.fit(X_train, y_train, epochs=100)

model.evaluate(X_test, y_test)

# defining parameters for the grid search
param_grid = {
    'hidden_layer_sizes': [(5, 5), (10, 10)],
    'activation': ['relu', 'tanh'],
    'batch_size': [16, 32],
    'epochs': [50, 100]
}

# creating an MLP classifier
mlp_classifier = KerasClassifier(build_fn = create_model, activation='relu',hidden_layer_sizes=(5, 5),verbose=0)

# using GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy')
grid_result = grid_search.fit(X_train, y_train)

# finding the best parameters
best_params = grid_result.best_params_
print(best_params)

#creating instance for a tuned model
tuned_model = create_model(
    activation=best_params['activation'],
    hidden_layer_sizes=best_params['hidden_layer_sizes']
    )

# tuning the model with the tuned hyperparameters and evaluating the model
tuned_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])
tuned_model.evaluate(X, y)

# Evaluating the model's accuracy on the test set
accuracy = tuned_model.evaluate(X_test, y_test)[1]
print(f'Tuned Model Accuracy: {accuracy}')
print()

# Predicting probabilities for the test set
y_pred_prob = tuned_model.predict(X_test)

# Calculating the AUC score
auc_score = roc_auc_score(y_test, y_pred_prob)
print(f'Tuned Model AUC Score: {auc_score}')
print()

"""**Saving the Model**
****
"""

# Save the functional tuned model
tuned_model.save('tuned_model.h5')

"""**Deploying the Model**
****
"""

!pip install Flask

import pickle
with open('scaler.pkl', 'wb') as scaler_file:
    pickle.dump(StandardScaler, scaler_file)